{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))  # Subtract max for stability(Cs 7015 - Nptel notes)\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function using cross entropy as it's a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_gradient(y_true, y_pred): # Needed for backpropagation\n",
    "    return (y_pred - y_true) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Params and forward pass and backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_sizes, output_size):\n",
    "    parameters = {}\n",
    "    layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "    num_layers = len(layer_sizes)\n",
    "\n",
    "    for i in range(1, num_layers):\n",
    "        # Xavier/He initialization for avoiding the vanishing/exploding gradient problem\n",
    "        parameters[f'W{i}'] = np.random.randn(layer_sizes[i - 1], layer_sizes[i]) * np.sqrt(2 / layer_sizes[i - 1])\n",
    "        parameters[f'b{i}'] = np.zeros((1, layer_sizes[i]))\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters, activation_func='relu'):\n",
    "    activations = {'A0': X}  # Store the input as the activation for the first layer\n",
    "    num_layers = len(parameters) // 2  \n",
    "\n",
    "    for i in range(1, num_layers):\n",
    "        W = parameters[f'W{i}']\n",
    "        b = parameters[f'b{i}']\n",
    "        Z = np.dot(activations[f'A{i-1}'], W) + b  # Pre-activation\n",
    "        activations[f'Z{i}'] = Z\n",
    "        if activation_func == 'relu':\n",
    "            A = relu(Z)\n",
    "        elif activation_func == 'sigmoid':\n",
    "            A = sigmoid(Z)\n",
    "        else:\n",
    "            raise ValueError(f\"Didnt select the activation function correctly: {activation_func}\")\n",
    "        activations[f'A{i}'] = A  # Store the activations for backprop\n",
    "\n",
    "    # Last layer uses softmax (for multi-class classification)\n",
    "    W = parameters[f'W{num_layers}']\n",
    "    b = parameters[f'b{num_layers}']\n",
    "    Z = np.dot(activations[f'A{num_layers-1}'], W) + b\n",
    "    activations[f'Z{num_layers}'] = Z\n",
    "    A = softmax(Z)\n",
    "    activations[f'A{num_layers}'] = A\n",
    "\n",
    "    return activations\n",
    "def backward_propagation(activations, parameters, y_true, activation_func='relu'):\n",
    "    gradients = {}\n",
    "    num_layers = len(parameters) // 2  # Number of weight matrices\n",
    "    m = y_true.shape[0]  # Number of samples\n",
    "\n",
    "    # Compute the gradient of the loss with respect to the output (A_L)\n",
    "    dA = cross_entropy_gradient(y_true, activations[f'A{num_layers}'])\n",
    "    dZ = dA # For softmax, dZ = dA\n",
    "\n",
    "    for i in range(num_layers, 0, -1):\n",
    "        dW = np.dot(activations[f'A{i-1}'].T, dZ)\n",
    "        db = np.sum(dZ, axis=0, keepdims=True)\n",
    "        gradients[f'dW{i}'] = dW\n",
    "        gradients[f'db{i}'] = db\n",
    "\n",
    "        if i > 1:\n",
    "            W = parameters[f'W{i}']\n",
    "            d_activation = None\n",
    "            if activation_func == 'relu':\n",
    "                d_activation = relu_derivative(activations[f'Z{i-1}'])\n",
    "            elif activation_func == 'sigmoid':\n",
    "                d_activation = sigmoid_derivative(activations[f'Z{i-1}'])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid activation function. Must be 'relu' or 'sigmoid'.\")\n",
    "            dZ = np.dot(dZ, W.T) * d_activation # Chain rule\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(parameters, gradients, learning_rate):\n",
    "    updated_parameters = {}\n",
    "    num_layers = len(parameters) // 2\n",
    "    for i in range(1, num_layers + 1):\n",
    "        W = parameters[f'W{i}']\n",
    "        b = parameters[f'b{i}']\n",
    "        dW = gradients[f'dW{i}']\n",
    "        db = gradients[f'db{i}']\n",
    "        updated_parameters[f'W{i}'] = W - learning_rate * dW\n",
    "        updated_parameters[f'b{i}'] = b - learning_rate * db\n",
    "    return updated_parameters, None\n",
    "\n",
    "def Momentum(parameters, gradients, learning_rate, momentum, v):\n",
    "    updated_parameters = {}\n",
    "    num_layers = len(parameters) // 2\n",
    "    for i in range(1, num_layers + 1):\n",
    "        W = parameters[f'W{i}']\n",
    "        b = parameters[f'b{i}']\n",
    "        dW = gradients[f'dW{i}']\n",
    "        db = gradients[f'db{i}']\n",
    "        if f'VdW{i}' not in v:\n",
    "            v[f'VdW{i}'] = np.zeros_like(dW)\n",
    "            v[f'Vdb{i}'] = np.zeros_like(db)\n",
    "        v[f'VdW{i}'] = momentum * v[f'VdW{i}'] + learning_rate * dW\n",
    "        v[f'Vdb{i}'] = momentum * v[f'Vdb{i}'] + learning_rate * db\n",
    "        updated_parameters[f'W{i}'] = W - v[f'VdW{i}']\n",
    "        updated_parameters[f'b{i}'] = b - v[f'Vdb{i}']\n",
    "    return updated_parameters, v\n",
    "\n",
    "def nag(parameters, gradients, learning_rate, momentum, v):\n",
    "    updated_parameters = {}\n",
    "    num_layers = len(parameters) // 2\n",
    "    for i in range(1, num_layers + 1):\n",
    "        W = parameters[f'W{i}']\n",
    "        b = parameters[f'b{i}']\n",
    "        dW = gradients[f'dW{i}']\n",
    "        db = gradients[f'db{i}']\n",
    "        if f'VdW{i}' not in v:\n",
    "            v[f'VdW{i}'] = np.zeros_like(dW)\n",
    "            v[f'Vdb{i}'] = np.zeros_like(db)\n",
    "\n",
    "        # Calculate the intermediate parameters (W_tilde, b_tilde)\n",
    "        W_tilde = W - momentum * v[f'VdW{i}']\n",
    "        b_tilde = b - momentum * v[f'Vdb{i}']\n",
    "\n",
    "        # Calculate gradients at the intermediate parameters\n",
    "        # Need to perform forward and backward propagation with W_tilde, b_tilde\n",
    "        intermediate_parameters = {f'W{k}': parameters[f'W{k}'] if k != i else W_tilde for k in range(1, num_layers + 1)}\n",
    "        intermediate_parameters = {f'b{k}': parameters[f'b{k}'] if k != i else b_tilde for k in range(1, num_layers + 1)}\n",
    "\n",
    "        #  Need a mini-forward and mini-backward prop here.\n",
    "        #   This is the key difference between Momentum and NAG.\n",
    "        # intermediate_gradients = gradients  # In practice, you'd calculate this with a forward/backward pass #remove this\n",
    "        activations = forward_propagation(X=X_train, parameters=intermediate_parameters, activation_func='relu') #add this\n",
    "        intermediate_gradients = backward_propagation(activations=activations, parameters=intermediate_parameters, y_true=y_train, activation_func='relu') #add this\n",
    "\n",
    "        v[f'VdW{i}'] = momentum * v[f'VdW{i}'] + learning_rate * intermediate_gradients[f'dW{i}']\n",
    "        v[f'Vdb{i}'] = momentum * v[f'Vdb{i}'] + learning_rate * intermediate_gradients[f'db{i}']\n",
    "        updated_parameters[f'W{i}'] = W - v[f'VdW{i}']\n",
    "        updated_parameters[f'b{i}'] = b - v[f'Vdb{i}']\n",
    "    return updated_parameters, v\n",
    "\n",
    "def adam(parameters, gradients, learning_rate, t, v):\n",
    "    updated_parameters = {}\n",
    "    num_layers = len(parameters) // 2\n",
    "    if v is None:\n",
    "        v = {}\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    for i in range(1, num_layers + 1):\n",
    "        W = parameters[f'W{i}']\n",
    "        b = parameters[f'b{i}']\n",
    "        dW = gradients[f'dW{i}']\n",
    "        db = gradients[f'db{i}']\n",
    "\n",
    "        if f'MdW{i}' not in v:\n",
    "            v[f'MdW{i}'] = np.zeros_like(dW)\n",
    "            v[f'Mdb{i}'] = np.zeros_like(db)\n",
    "            v[f'VdW{i}'] = np.zeros_like(dW)\n",
    "            v[f'Vdb{i}'] = np.zeros_like(db)\n",
    "\n",
    "        v[f'MdW{i}'] = beta1 * v[f'MdW{i}'] + (1 - beta1) * dW\n",
    "        v[f'Mdb{i}'] = beta1 * v[f'Mdb{i}'] + (1 - beta1) * db\n",
    "        v[f'VdW{i}'] = beta2 * v[f'VdW{i}'] + (1 - beta2) * (dW ** 2)\n",
    "        v[f'Vdb{i}'] = beta2 * v[f'Vdb{i}'] + (1 - beta2) * (db ** 2)\n",
    "\n",
    "        MdW = v[f'MdW{i}'] / (1 - beta1 ** t)\n",
    "        Mdb = v[f'Mdb{i}'] / (1 - beta1 ** t)\n",
    "        VdW = v[f'VdW{i}'] / (1 - beta2 ** t)\n",
    "        Vdb = v[f'Vdb{i}'] / (1 - beta2 ** t)\n",
    "\n",
    "        updated_W = W - learning_rate * MdW / (np.sqrt(VdW) + epsilon)\n",
    "        updated_b = b - learning_rate * Mdb / (np.sqrt(Vdb) + epsilon)\n",
    "        updated_parameters[f'W{i}'] = updated_W\n",
    "        updated_parameters[f'b{i}'] = updated_b\n",
    "    return updated_parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate, optimizer='gd', momentum=0.5, v=None, t=None):\n",
    "    if v is None:\n",
    "        v = {}\n",
    "\n",
    "    if optimizer == 'gd':\n",
    "        return gd(parameters, gradients, learning_rate)\n",
    "    elif optimizer == 'momentum':\n",
    "        return Momentum(parameters, gradients, learning_rate, momentum, v)\n",
    "    elif optimizer == 'nag':\n",
    "        return nag(parameters, gradients, learning_rate, momentum, v)\n",
    "    elif optimizer == 'adam':\n",
    "        return adam(parameters, gradients, learning_rate, t, v)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid optimizer: {optimizer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 64, Hidden sizes: [128, 64, 128, 64], Output size: 10\n",
      "Epoch 1/50, Loss: 1.0945, Val Loss: 1.1682\n",
      "Epoch 2/50, Loss: 0.4690, Val Loss: 0.5657\n",
      "Epoch 3/50, Loss: 0.2254, Val Loss: 0.3160\n",
      "Epoch 4/50, Loss: 0.1312, Val Loss: 0.2389\n",
      "Epoch 5/50, Loss: 0.0849, Val Loss: 0.2038\n",
      "Epoch 6/50, Loss: 0.0588, Val Loss: 0.1884\n",
      "Epoch 7/50, Loss: 0.0415, Val Loss: 0.1774\n",
      "Epoch 8/50, Loss: 0.0301, Val Loss: 0.1706\n",
      "Epoch 9/50, Loss: 0.0222, Val Loss: 0.1661\n",
      "Epoch 10/50, Loss: 0.0173, Val Loss: 0.1660\n",
      "Epoch 11/50, Loss: 0.0140, Val Loss: 0.1668\n",
      "Epoch 12/50, Loss: 0.0118, Val Loss: 0.1671\n",
      "Epoch 13/50, Loss: 0.0102, Val Loss: 0.1678\n",
      "Epoch 14/50, Loss: 0.0090, Val Loss: 0.1684\n",
      "Epoch 15/50, Loss: 0.0081, Val Loss: 0.1693\n",
      "Epoch 16/50, Loss: 0.0074, Val Loss: 0.1700\n",
      "Epoch 17/50, Loss: 0.0068, Val Loss: 0.1707\n",
      "Early stopping at epoch 17\n",
      "Best validation loss: 0.1660 at epoch 10\n",
      "Test Loss: 0.0904, Test Accuracy: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99        33\n",
      "           1       0.97      1.00      0.98        28\n",
      "           2       0.97      0.97      0.97        33\n",
      "           3       1.00      0.94      0.97        34\n",
      "           4       1.00      1.00      1.00        46\n",
      "           5       0.96      0.98      0.97        47\n",
      "           6       0.97      0.97      0.97        35\n",
      "           7       1.00      0.97      0.99        34\n",
      "           8       0.90      0.93      0.92        30\n",
      "           9       0.97      0.95      0.96        40\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "[[33  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 28  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  1  0]\n",
      " [ 0  0  1 32  0  1  0  0  0  0]\n",
      " [ 0  0  0  0 46  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 46  1  0  0  0]\n",
      " [ 1  0  0  0  0  0 34  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 33  0  1]\n",
      " [ 0  1  0  0  0  1  0  0 28  0]\n",
      " [ 0  0  0  0  0  0  0  0  2 38]]\n",
      "Trained model saved to final_model.pkl\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    m = y.shape[0]\n",
    "    one_hot = np.zeros((m, num_classes))\n",
    "    one_hot[np.arange(m), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def load_data(path_to_train=None, path_to_val=None, path_to_test=None):\n",
    "    if path_to_train and path_to_val:\n",
    "        # Load from CSV files\n",
    "        train_df = pd.read_csv(path_to_train)\n",
    "        val_df = pd.read_csv(path_to_val)\n",
    "        X_train = train_df.iloc[:, :-1].values  # All columns except the last one\n",
    "        y_train = train_df.iloc[:, -1].values   # Last column\n",
    "        X_val = val_df.iloc[:, :-1].values\n",
    "        y_val = val_df.iloc[:, -1].values\n",
    "        X_test = None\n",
    "        y_test = None\n",
    "        if path_to_test:\n",
    "            test_df = pd.read_csv(path_to_test)\n",
    "            X_test = test_df.iloc[:, :-1].values\n",
    "            y_test = test_df.iloc[:, -1].values\n",
    "    else:\n",
    "        # Load the digits dataset from scikit-learn and split it\n",
    "        digits = load_digits()\n",
    "        X, y = digits.data, digits.target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42) # 0.25 of the original training set is now validation\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    if X_test is not None:\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def train(X_train, y_train, X_val, y_val,\n",
    "          learning_rate=0.001, momentum=0.5, num_hidden=1, sizes=\"64\",\n",
    "          activation='relu', loss='ce', opt='adam', batch_size=20,\n",
    "          num_epochs=10, anneal=False, path_save_dir='models/',\n",
    "          expt_dir='logs/', to_pretrain=False, epoch_to_restore=0,\n",
    "          to_test=False, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Trains the neural network model.\n",
    "\n",
    "    Args:\n",
    "        X_train (numpy.ndarray): Training data.\n",
    "        y_train (numpy.ndarray): Training labels.\n",
    "        X_val (numpy.ndarray): Validation data.\n",
    "        y_val (numpy.ndarray): Validation labels.\n",
    "        learning_rate (float, optional): Learning rate. Defaults to 0.001.\n",
    "        momentum (float, optional): Momentum. Defaults to 0.5.\n",
    "        num_hidden (int, optional): Number of hidden layers. Defaults to 1.\n",
    "        sizes (str, optional): Sizes of hidden layers (comma-separated). Defaults to \"64\".\n",
    "        activation (str, optional): Activation function. Defaults to 'relu'.\n",
    "        loss (str, optional): Loss function ('ce'). Defaults to 'ce'.\n",
    "        opt (str, optional): Optimizer to use ('gd', 'momentum', 'adam', 'nag').\n",
    "        batch_size (int, optional): Batch size. Defaults to 20.\n",
    "        num_epochs (int, optional): Number of epochs. Defaults to 10.\n",
    "        anneal (bool, optional): Whether to use annealing. Defaults to False.\n",
    "        path_save_dir (str, optional): Path to save models. Defaults to 'models/'.\n",
    "        expt_dir (str, optional): Path to save logs. Defaults to 'logs/'.\n",
    "        to_pretrain (bool, optional): Whether to pretrain. Defaults to False.\n",
    "        epoch_to_restore (int, optional): Epoch to restore from. Defaults to 0.\n",
    "        to_test (bool, optional): Whether to test. Defaults to False.\n",
    "        X_test (numpy.ndarray, optional): Test data. Defaults to None.\n",
    "        y_test (numpy.ndarray, optional): Test labels. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(path_save_dir, exist_ok=True)\n",
    "    os.makedirs(expt_dir, exist_ok=True)\n",
    "\n",
    "    # Save hyperparameters to a file\n",
    "    with open(os.path.join(expt_dir, 'readme.txt'), 'w') as f:\n",
    "        f.write(f\"learning_rate: {learning_rate}\\n\")\n",
    "        f.write(f\"momentum: {momentum}\\n\")\n",
    "        f.write(f\"num_hidden: {num_hidden}\\n\")\n",
    "        f.write(f\"sizes: {sizes}\\n\")\n",
    "        f.write(f\"activation: {activation}\\n\")\n",
    "        f.write(f\"loss: {loss}\\n\")\n",
    "        f.write(f\"opt: {opt}\\n\")\n",
    "        f.write(f\"batch_size: {batch_size}\\n\")\n",
    "        f.write(f\"num_epochs: {num_epochs}\\n\")\n",
    "        f.write(f\"anneal: {anneal}\\n\")\n",
    "        f.write(f\"path_save_dir: {path_save_dir}\\n\")\n",
    "        f.write(f\"expt_dir: {expt_dir}\\n\")\n",
    "        f.write(f\"to_pretrain: {to_pretrain}\\n\")\n",
    "        f.write(f\"epoch_to_restore: {epoch_to_restore}\\n\")\n",
    "        f.write(f\"to_test: {to_test}\\n\")\n",
    "\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_sizes = [int(s) for s in sizes.split(',')] * num_hidden  # Repeat if necessary\n",
    "    output_size = len(np.unique(y_train))\n",
    "    num_classes = output_size # For one-hot encoding\n",
    "\n",
    "    print(f\"Input size: {input_size}, Hidden sizes: {hidden_sizes}, Output size: {output_size}\") # Debug\n",
    "\n",
    "    if to_pretrain:\n",
    "        # Load pre-trained weights\n",
    "        try:\n",
    "            with open(os.path.join(path_save_dir, f'weights_epoch_{epoch_to_restore}.pkl'), 'rb') as f:\n",
    "                parameters = pickle.load(f)\n",
    "            print(f\"Loaded pre-trained weights from epoch {epoch_to_restore}\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Pre-trained weights not found.  Training from scratch.\")\n",
    "            parameters = initialize_parameters(input_size, hidden_sizes, output_size)\n",
    "    else:\n",
    "        # Initialize weights and biases\n",
    "        parameters = initialize_parameters(input_size, hidden_sizes, output_size)\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    y_train_encoded = one_hot_encode(y_train, num_classes)\n",
    "    y_val_encoded = one_hot_encode(y_val, num_classes)\n",
    "\n",
    "    # Initialize velocity for momentum, NAG, and Adam\n",
    "    v = None\n",
    "    t = 0 # For Adam\n",
    "\n",
    "    # Best validation loss and epoch for early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Mini-batch gradient descent\n",
    "        num_batches = X_train.shape[0] // batch_size\n",
    "        for batch in range(num_batches):\n",
    "            t += 1 # For Adam\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train_encoded[start:end]\n",
    "\n",
    "            # Forward propagation\n",
    "            activations = forward_propagation(X_batch, parameters, activation)\n",
    "            y_pred_batch = activations[f'A{len(hidden_sizes) + 1}'] #get the last activation\n",
    "\n",
    "            # Compute loss\n",
    "            loss_batch = cross_entropy_loss(y_batch, y_pred_batch)\n",
    "\n",
    "            # Backward propagation\n",
    "            gradients = backward_propagation(activations, parameters, y_batch, activation)\n",
    "\n",
    "            # Update parameters\n",
    "            parameters, v = update_parameters(parameters, gradients, learning_rate, opt, momentum, v, t)\n",
    "\n",
    "        # Compute training and validation loss for the epoch\n",
    "        activations_train = forward_propagation(X_train, parameters, activation)\n",
    "        y_pred_train = activations_train[f'A{len(hidden_sizes) + 1}']\n",
    "        loss_train = cross_entropy_loss(y_train_encoded, y_pred_train)\n",
    "\n",
    "        activations_val = forward_propagation(X_val, parameters, activation)\n",
    "        y_pred_val = activations_val[f'A{len(hidden_sizes) + 1}']\n",
    "        loss_val = cross_entropy_loss(y_val_encoded, y_pred_val)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss_train:.4f}, Val Loss: {loss_val:.4f}\")\n",
    "\n",
    "        # Save model weights\n",
    "        with open(os.path.join(path_save_dir, f'weights_epoch_{epoch + 1}.pkl'), 'wb') as f:\n",
    "            pickle.dump(parameters, f)\n",
    "\n",
    "        # Early stopping\n",
    "        if loss_val < best_val_loss:\n",
    "            best_val_loss = loss_val\n",
    "            best_epoch = epoch + 1\n",
    "        elif epoch - best_epoch > 5:  # Stop if validation loss hasn't improved in 5 epochs\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        # Annealing \n",
    "        if anneal:\n",
    "            learning_rate *= 0.95  # Reduce learning rate each epoch\n",
    "\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "    # Evaluate on the test set if provided\n",
    "    if to_test and X_test is not None and y_test is not None:\n",
    "        y_test_encoded = one_hot_encode(y_test, num_classes)\n",
    "        activations_test = forward_propagation(X_test, parameters, activation)\n",
    "        y_pred_test = activations_test[f'A{len(hidden_sizes) + 1}']\n",
    "        loss_test = cross_entropy_loss(y_test_encoded, y_pred_test)\n",
    "        y_pred_test_labels = np.argmax(y_pred_test, axis=1)  # Convert one-hot to labels\n",
    "        accuracy = accuracy_score(y_test, y_pred_test_labels)\n",
    "        print(f\"Test Loss: {loss_test:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Print classification report and confusion matrix\n",
    "        print(classification_report(y_test, y_pred_test_labels))\n",
    "        print(confusion_matrix(y_test, y_pred_test_labels))\n",
    "    return parameters #return the trained parameters\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set your parameters\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    num_hidden = 2\n",
    "    sizes = \"128,64\"\n",
    "    activation = 'relu'  # or 'sigmoid'\n",
    "    loss = 'ce'  #  'ce' for cross-entropy\n",
    "    opt = 'adam'  # 'gd', 'momentum', 'adam', 'nag'\n",
    "    batch_size = 64\n",
    "    num_epochs = 50\n",
    "    anneal = False\n",
    "    path_save_dir = 'models/'\n",
    "    expt_dir = 'logs/'\n",
    "    to_pretrain = False\n",
    "    epoch_to_restore = 0\n",
    "    to_test = True # Set to true if you have a test set\n",
    "    path_to_train = None  # Or path to your training CSV\n",
    "    path_to_val = None    # Or path to your validation CSV\n",
    "    path_to_test = None   # Or path to your test CSV\n",
    "\n",
    "    # Load data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_data(path_to_train, path_to_val, path_to_test)\n",
    "\n",
    "    # Train the model\n",
    "    trained_parameters = train(X_train, y_train, X_val, y_val,\n",
    "                      learning_rate, momentum, num_hidden, sizes,\n",
    "                      activation, loss, opt, batch_size, num_epochs,\n",
    "                      anneal, path_save_dir, expt_dir, to_pretrain,\n",
    "                      epoch_to_restore, to_test, X_test, y_test)\n",
    "    # Save trained model\n",
    "    with open(os.path.join(path_save_dir, 'final_model.pkl'), 'wb') as f:\n",
    "        pickle.dump(trained_parameters, f)\n",
    "    print(\"Trained model saved to final_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
